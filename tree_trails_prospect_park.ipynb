{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Tree Trails of Prospect Park\n",
    "\n",
    "\n",
    "\n",
    "This notebook walks you through the process of using linked data (with [Wikidata](https://www.wikidata.org)) and natural language processing (with [SpaCy](https://spacy.io/)) to enrich a 1968 book of walking tours of Prospect Park trees, [Tree Trails in Prospect Park](https://www.echonyc.com/~parks/books/treetrailsppk.html), with images and species information for easier identification. It outputs two CSV files suitable for import into two linked [Memento](https://mementodatabase.com/) databases (Android or desktop only). One database will contain information on the tree species (taxon name, common names, links to Wikipedia, Wikimedia Commons for images, and iNaturalist) and the other will contain the stops on the tours linked to the trees featured, with editable fields for you to store your location when you find the tree and any photos you take. Alternatively, you can output two CSV files to create an [Airtable](https://airtable.com/) database (change `formatting` variable to 'airtable' under the \"Generating tour stops...\" block), though setup is a little more involved and Airtable does not support a geographic coordinates datatype, so you will not be able to store your location. It also doesn't support rich text for data imported through CSV, so you won't see tree species highlighted in the text.\n",
    "\n",
    "This process is broken out into the following stages:\n",
    "1. Get a list of all possible tree species and related information from Wikipedia and Wikidata\n",
    "2. Use the SpaCy natural language programming library to locate instances of these trees in the text of the book\n",
    "3. Reshape the text of the book with the tree information into an interactive database \n",
    "\n",
    "You will need to install the following Python libraries (all available with pip):\n",
    "- requests\n",
    "- lxml\n",
    "- jsonlines\n",
    "- spacy\n",
    "\n",
    "If you want to skip directly to installing and loading the data, follow the instructions in [importing-database-data.md](importing-database-data.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may need to install the requests, lxml, jsonlines, and spacy libraries before you start. All can be installed with pip.\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from lxml import etree as et\n",
    "import jsonlines\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of tree species\n",
    "\n",
    "There isn't a great way to query Wikidata for all tree species, so I scraped all listed species from this [Wikpedia list of trees and shrubs by taxonomic family](https://en.wikipedia.org/wiki/List_of_trees_and_shrubs_by_taxonomic_family) using lxml's xpath() function to get the Wikipedia article titles and species names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use lxml's HTMLParser to put the html content into a searchable tree structure\n",
    "parser = et.HTMLParser()\n",
    "html = requests.get('https://en.wikipedia.org/wiki/List_of_trees_and_shrubs_by_taxonomic_family').content\n",
    "tree = et.fromstring(html, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit results to all table row (<tr>) elements in the html\n",
    "rows = tree.xpath('//tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Within each <tr>, the first table data (<td>) element contains the species information we need, so add each of those to a list\n",
    "species = []\n",
    "for row in rows:\n",
    "    if len(row.xpath('td')) > 0:\n",
    "        s = row.xpath('./td')[0]\n",
    "        species.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Within each of these <td> elements, the species name and Wikipedia links are in the @title and @href attributes\n",
    "tree_species = []\n",
    "for s in species:\n",
    "    if len(s.xpath('a/@href')) > 0:\n",
    "        #extract the name and wiki_link from each of the results matching the xpath above\n",
    "        ts = {}\n",
    "        ts['name'] = s.xpath('a/@title')[0]\n",
    "        ts['wiki_link'] = s.xpath('a/@href')[0]\n",
    "        #except some of these pages are not species, so skip those\n",
    "        if ts['name'] not in ['Least-concern species', 'Vulnerable species', 'Endangered species', 'Critically endangered']:\n",
    "            tree_species.append(ts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Wikidata ids for each tree species\n",
    "\n",
    "With the Wikipedia article names from the scraped list, you can use the Wikipedia API to get the corresponding Wikidata id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWikidataId(wp_id):\n",
    "    \"\"\"Get Wikidata id for a given Wikipedia article title\"\"\"\n",
    "    base = 'https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&ppprop=wikibase_item&redirects=1&format=json&titles='\n",
    "    url = base + wp_id\n",
    "    query = requests.get(url)\n",
    "    wd_id = None\n",
    "    response = json.loads(query.content)\n",
    "    for k, v in response['query']['pages'].items():\n",
    "        wd_id = v['pageprops']['wikibase_item']\n",
    "    return wd_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the API to get the corresponding wikidata id and add it to the tree species entry\n",
    "for ts in tree_species:\n",
    "    base = 'https://en.wikipedia.org'\n",
    "    #some trees do not have wikipedia pages, so ignore these\n",
    "    if 'page does not exist' not in ts['name']:\n",
    "        ts['wikidata'] = getWikidataId(ts['name'])\n",
    "    #Use a 1 second rate limit in between queries\n",
    "    time.sleep(1)\n",
    "\n",
    "#saving as we go\n",
    "f = open('tree_species.json', 'w')\n",
    "json.dump(tree_species, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some species in our text aren't in this list because they are cultivars or technically not trees, so we'll add them now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "addtl_wiki_ids = [{'name': \"Ulmus glabra 'Camperdownii'\", 'wikidata': 'Q7879447'},\n",
    "{'name': 'Clethra alnifolia', 'wikidata': 'Q5131966'},\n",
    "{'name': 'Picea orientalis', 'wikidata': 'Q1145286'},\n",
    "{'name': 'Pinus densiflora umbraculifera', 'wikidata': 'Q74534097'},\n",
    "{'name': 'Ulmus carpinifolia', 'wikidata': 'Q3547946'},\n",
    "{'name': 'Ilex crenata', 'wikidata': 'Q1328685'},\n",
    "{'name': 'Euonymus kiautschovica', 'wikidata': 'Q15226197'},\n",
    "{'name': 'Magnolia soulangeana', 'wikidata': 'Q731443'},\n",
    "{'name': 'Aesculus carnea', 'wikidata': 'Q163779'}]\n",
    "\n",
    "tree_species.extend(addtl_wiki_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate this list, just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_species = [dict(t) for t in {tuple(d.items()) for d in tree_species}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Wikidata info on Species: name, common names, Wikipedia Commons link, and iNaturalist id\n",
    "\n",
    "With the Wikidata ids, use SPARQL to [https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial](query Wikidata) at the [https://query.wikidata.org/](Wikidata Query Service (WDQS)) for each, retrieving species name, alt label, common names, Wikimedia Commons page (useful for images), and iNaturalist id (for more info and local observations of the species). You can get any ids you want from the Wikidata page, such as NCBI taxonomy ID, USDA Plants ID, or many more. I chose iNaturalist because of the easy interface to photos, commmon names, and local observations. If you want to get additional identifiers or properties back in your query, you can adjust the query below by adding a statement to the WHERE clause similar to `OPTIONAL {{ {} wdt:P3151 ?inaturalist. }}` where `P3151` is the property you want retrieve and `?inaturalist` is a variable name of your choice to represent the property value. Append \"Label\" to the end of this variable and add it to the SELECT clause to return the value in your query results, (ex. `?inaturalistLabel`). The \"OPTIONAL\" clause ensures that all of the other results your requesting for the species will be returned even if the value of this property isn't present.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWikidataBySpecies(request_id):\n",
    "    \"\"\"Function to retrive items and properties by tree species id through the WDQS.\"\"\"\n",
    "    #Add additional properties within the select clause as desired\n",
    "    wdid = 'wd:' + request_id\n",
    "    #doubled curly braces are used here instead of single because you're sending the query using REST\n",
    "    sparql = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            SELECT ?label ?altLabel ?commonLabel ?inaturalistLabel ?wpcommonsLabel\n",
    "            WHERE \n",
    "            {{\n",
    "              {} rdfs:label ?label .\n",
    "                FILTER (langMatches( lang(?label), \"EN\" ) )\n",
    "              OPTIONAL {{ {} skos:altLabel ?altLabel FILTER ( lang(?altLabel) = \"en\" ). }}\n",
    "              OPTIONAL {{ {} wdt:P3151 ?inaturalist. }}\n",
    "              OPTIONAL {{ {} wdt:P1843 ?common filter (lang(?common) = \"en\").}}\n",
    "              OPTIONAL {{ {} wdt:P935 ?wpcommons. }}\n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "            }}\n",
    "            \"\"\".format(wdid, wdid, wdid, wdid, wdid)\n",
    "    base = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    query = requests.post(base, headers=headers, params={'query': sparql, 'format': 'json'})\n",
    "    #store and return the request information and data in a dict\n",
    "    request = {}\n",
    "    request['request_id'] = request_id\n",
    "    request['sparql_query'] = sparql\n",
    "    request['status_code'] = query.status_code\n",
    "    if query.status_code == 200:\n",
    "        request['data'] = query.json()\n",
    "    else:\n",
    "        request['data'] = None\n",
    "    return request\n",
    "\n",
    "def parseWikidataBySpecies(response):\n",
    "    \"\"\"Parse the wikidata response and put it into a more readable dict\"\"\"\n",
    "    results = {}\n",
    "    #add results from each binding to list for each property, then dedupe each list before returning value\n",
    "    for b in response['data']['results']['bindings']:\n",
    "        for k, v in b.items():\n",
    "            if k in results:\n",
    "                results[k].append(v['value'])\n",
    "            else:\n",
    "                results[k] = [v['value']]\n",
    "    for k, v in results.items():\n",
    "        results[k] = list(set(v))\n",
    "    return results\n",
    "\n",
    "def reshapeWikidata(tree_species):\n",
    "    \"\"\"Reshape all of the tree species data into more usable data\"\"\"\n",
    "    for ts in tree_species:\n",
    "        #expand links into full URLs\n",
    "        if 'wiki_link' in ts:\n",
    "            ts['wikipedia'] = 'https://en.wikipedia.org/' + ts['wiki_link']\n",
    "        #remove wikipedia links and '(page does not exist)' from species without wikipedia pages\n",
    "        if '(page does not exist)' in ts['name']:\n",
    "            ts.pop('wikipedia')\n",
    "            ts['name'] = ts['name'].replace(' (page does not exist)', '')\n",
    "        if 'raw_wd' in ts:\n",
    "            wd_data = parseWikidataBySpecies(ts['raw_wd'])\n",
    "            #make the inaturalist ids and WM commons ids into urls\n",
    "            if 'inaturalistLabel' in wd_data:\n",
    "                ts['inaturalist'] = 'https://inaturalist.org/taxa/' + wd_data['inaturalistLabel'][0]\n",
    "            if 'wpcommonsLabel' in wd_data:\n",
    "                ts['wp_commons'] = 'https://commons.wikimedia.org/wiki/' + wd_data['wpcommonsLabel'][0]\n",
    "            if 'label' in wd_data:\n",
    "                ts['species'] = wd_data['label'][0]\n",
    "            #combine altLabels and common names into a single list\n",
    "            ts['common_names'] = []\n",
    "            if 'altLabel' in wd_data:\n",
    "                ts['common_names'].extend(wd_data['altLabel'])\n",
    "            if 'commonLabel' in wd_data:\n",
    "                ts['common_names'].extend(wd_data['commonLabel'])\n",
    "            #convert all common names to title case and dedupe\n",
    "            ts['common_names'] = [c.title() for c in ts['common_names']]\n",
    "            ts['common_names'] = list(set(ts['common_names']))\n",
    "    return tree_species\n",
    "\n",
    "def writeSpeciesToCsv(tree_species):\n",
    "    \"\"\"Write the data out to a csv file suitable for import into a database\"\"\"\n",
    "    c = open('tree_species.csv', 'w')\n",
    "    fieldnames = ['Species', 'Common names', 'Wikipedia', 'Wikimedia Commons', 'iNaturalist']\n",
    "    writer = csv.DictWriter(c, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for ts in tree_species:\n",
    "        row = {}\n",
    "        row['Species'] = ts['name']\n",
    "        if 'common_names' in ts:\n",
    "            row['Common names'] = ', '.join(ts['common_names'])\n",
    "        if 'wikipedia' in ts:\n",
    "            row['Wikipedia'] = ts['wikipedia']\n",
    "        if 'wp_commons' in ts:\n",
    "            row['Wikimedia Commons'] = ts['wp_commons']\n",
    "        if 'inaturalist' in ts:\n",
    "            row['iNaturalist'] = ts['inaturalist']\n",
    "        writer.writerow(row)\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query wikidata for each species (using a respectable rate limit).\n",
    "for ts in tree_species:\n",
    "    if 'wikidata' in ts:\n",
    "        wd = getWikidataBySpecies(ts['wikidata'])\n",
    "        ts['raw_wd'] = wd\n",
    "        time.sleep(1)\n",
    "\n",
    "#saving as we go        \n",
    "f = open('tree_species.json', 'w')\n",
    "json.dump(tree_species, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the raw Wikidata results into a more structured form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_species = reshapeWikidata(tree_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common names and alternate species names are in the Tree Trails text but not our tree species list, so we'll add them here. (I added quite a few to Wikidata as I was working on this project, but I didn't add any that seemed like they weren't commonly accepted common names, i.e. not in the Wikipedia article or on the first page of a google search.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "addtl_names = [{'wikidata': 'Q1328685', 'alt_species': 'Ilex crenata convexa'},\n",
    "{'wikidata': 'Q161374', 'alt_species': 'Planatus acerifolia'},\n",
    "{'wikidata': 'Q288558', 'alt_species': 'Sophora'},\n",
    "{'wikidata': 'Q161166', 'common_name': 'Box elder'},\n",
    "{'wikidata': 'Q161382', 'common_name': 'Silver-leafed linden'},\n",
    "{'wikidata': 'Q147525', 'common_name': 'Low-branched red oak'},\n",
    "{'wikidata': 'Q165137', 'common_name': 'European cherry'},\n",
    "{'wikidata': 'Q3125935', 'common_name': 'Two-trunked silverbell'},\n",
    "{'wikidata': 'Q26745', 'common_name': 'Schwedler maple'},\n",
    "{'wikidata': 'Q549418', 'common_name': 'Kentucky coffee tree'},\n",
    "{'wikidata': 'Q1328685', 'common_name': 'Boxleaf holly'},\n",
    "{'wikidata': 'Q74534097', 'common_name': 'Tanyosho pine'},\n",
    "{'wikidata': 'Q7879447', 'common_name': 'Camperdown elm'},\n",
    "{'wikidata': 'Q5131966', 'common_name': 'Sweet pepperbush'},\n",
    "{'wikidata': 'Q1145286', 'common_name': 'Oriental spruce'},\n",
    "{'wikidata': 'Q74534097', 'common_name': 'Japanese umbrella pine'},\n",
    "{'wikidata': 'Q3547946', 'common_name': 'Smooth-leaved elm'},\n",
    "{'wikidata': 'Q1683340', 'common_name': 'Chinese tree lilac'},\n",
    "{'wikidata': 'Q1328685', 'common_name': 'Japanese holly'},\n",
    "{'wikidata': 'Q731443', 'common_name': 'Saucer magnolia'},\n",
    "{'wikidata': 'Q26899', 'common_name': 'European horsechestnut'},\n",
    "{'wikidata': 'Q288558', 'common_name': 'Pagoda tree'},\n",
    "{'wikidata': 'Q3547946', 'common_name': 'Smooth-leafed elm'},\n",
    "{'wikidata': 'Q158746', 'common_name': 'Small-leafed linden'},\n",
    "{'wikidata': 'Q158746', 'common_name': 'Small-leaf European linden'},              \n",
    "{'wikidata': 'Q163760', 'common_name': 'European linden'},\n",
    "{'wikidata': 'Q157230', 'common_name': 'White pine'},\n",
    "{'wikidata': 'Q549418', 'alt_species': 'Gymnocladus dioicus'},\n",
    "{'wikidata': 'Q24877919', 'common_name': 'Chinese tree lilac'},\n",
    "{'wikidata': 'Q163779', 'common_name': 'Pink-flowering horsechestnut'},\n",
    "{'wikidata': 'Q161374', 'alt_species': 'Platanus acerifolia'},\n",
    "{'wikidata': 'Q470006', 'common_name': 'Hackberry'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in addtl_names:\n",
    "    for t in tree_species:\n",
    "        if 'wikidata' in t:\n",
    "            if a['wikidata'] == t['wikidata']:\n",
    "                if 'common_name' in a:\n",
    "                    if 'common_names' in t:\n",
    "                        t['common_names'].append(a['common_name'])\n",
    "                    else:\n",
    "                        t['common_names'] = [a['common_name']]\n",
    "                elif 'alt_species' in a:\n",
    "                    t['alt_species'] = [a['alt_species']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some names from Wikidata are duplicated across species and will result in associating the wrong tree with a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_name = [{'wikidata': 'Q1988747', 'common_name': 'White pine'},\n",
    "{'wikidata': 'Q1981615', 'common_name': 'White pine'},\n",
    "{'wikidata': 'Q2724971', 'species': 'Prunus nigra'},\n",
    "{'wikidata': 'Q147064', 'species': 'Populus nigra'},\n",
    "{'wikidata': 'Q1683340', 'species': 'Syringa reticulata'},\n",
    "{'wikidata': 'Q1981615', 'species': 'Dacrycarpus dacrydioides'},\n",
    "{'wikidata': 'Q27344', 'species': 'Buxus hyrcana'},\n",
    "{'wikidata': 'Q158987', 'species': 'Prunus salicifolia'}]\n",
    "\n",
    "entity_remove = []\n",
    "for i, ts in enumerate(tree_species):\n",
    "    for r in remove_name:\n",
    "        if 'wikidata' in ts:\n",
    "            if ts['wikidata'] == r['wikidata']:\n",
    "                if 'common_name' in r:\n",
    "                    ts['common_names'] = [c for c in ts['common_names'] if c != r['common_name']]\n",
    "                if 'species'in r:\n",
    "                    if r['species'] == ts['name']:\n",
    "                        entity_remove.append(i)\n",
    "tree_species = [ts for i, ts in enumerate(tree_species) if i not in entity_remove]\n",
    "# tree_species = [ts for ts in tree_species if ('wikidata' not in ts) or ('wikidata' in ts and ts['wikidata'] not in entity_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract species from Tree Trails text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data matching rules\n",
    "Make a SpaCy patterns.jsonl file from the tree species data with all possible variations on the text strings we want to extract from the book text--singular or plural common names, full or abbreviated species names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluralize(text):\n",
    "    \"\"\"Get the plural form of a singular noun\"\"\"\n",
    "    if text.endswith('y'):\n",
    "        text = text.replace('y', 'ies')\n",
    "    elif text.endswith(('ch', 's', 'sh', 'z', 'x')):\n",
    "        text = text + 'es'\n",
    "    else:\n",
    "        text = text + 's'\n",
    "    return text\n",
    "\n",
    "def tokenHyphen(text):\n",
    "    \"\"\"Tokenize hyphenated words into SpaCy patterns\"\"\"\n",
    "    tokens = text.split('-')\n",
    "    patterns = []\n",
    "    pattern1 = {}\n",
    "    pattern1['LOWER'] = tokens[0].lower()\n",
    "    patterns.append(pattern1)\n",
    "    hyphen = {}\n",
    "    hyphen['ORTH'] = '-'\n",
    "    patterns.append(hyphen)\n",
    "    pattern2 = {}\n",
    "    pattern2['LOWER'] = tokens[1].lower()\n",
    "    patterns.append(pattern2)\n",
    "    return patterns\n",
    "\n",
    "def constructTerm(term, label, id):\n",
    "    \"\"\"Create all the patterns needed for matching any variations on a tree species name\"\"\"\n",
    "    termlist = []\n",
    "    if term != '': \n",
    "        listitem = {}\n",
    "        listitem['label'] = label\n",
    "        listitem['id'] = id\n",
    "        patterns = []\n",
    "    #for labels that are tree species or alternate names for tree species\n",
    "    if label in ['TREE SPECIES', 'ALT TREE SPECIES']:\n",
    "        for s in term.split(' '):\n",
    "            pattern = {}\n",
    "            #we will lowercase all words in the text and in patterns so we don't have to worry abut case matching\n",
    "            pattern['LOWER'] = s.lower()\n",
    "            patterns.append(pattern)\n",
    "        listitem['pattern'] = patterns\n",
    "        termlist.append(listitem.copy())\n",
    "        #create a pattern with genus abbreviated, ex. \"p. strobus\"\n",
    "        altitem = {}\n",
    "        altitem['label'] = label\n",
    "        altitem['id'] = id\n",
    "        altpatterns = []\n",
    "        for i, s in enumerate(term.split(' ')):\n",
    "            altpattern = {}\n",
    "            if i == 0:\n",
    "                altpattern['LOWER'] = s[0].lower() + '.'\n",
    "                altpatterns.append(altpattern)\n",
    "            else:\n",
    "                altpattern['LOWER'] = s.lower()\n",
    "                altpatterns.append(altpattern)\n",
    "        altitem['pattern'] = altpatterns\n",
    "        termlist.append(altitem.copy())\n",
    "    #for the labels that are common names, add patterns for matching pluralized form in addition to singular \n",
    "    elif label == 'TREE COMMON NAME':\n",
    "        for i, s in enumerate(term.split(' ')):\n",
    "            if '-' in s:\n",
    "                hyphenpatterns = tokenHyphen(s)\n",
    "                patterns.extend(hyphenpatterns)\n",
    "            else:\n",
    "                pattern = {}\n",
    "                pattern['LOWER'] = s.lower()\n",
    "                patterns.append(pattern)\n",
    "        listitem['pattern'] = patterns\n",
    "        termlist.append(listitem.copy())\n",
    "        patterns = []\n",
    "        for i, s in enumerate(term.split()):\n",
    "            pattern = {}\n",
    "            #pluralize only the last token in the word\n",
    "            if i != len(term.split())-1:\n",
    "                if '-' in s:\n",
    "                    hyphenpatterns = tokenHyphen(s)\n",
    "                    patterns.extend(hyphenpatterns)\n",
    "                else:\n",
    "                    pattern['LOWER'] = s.lower()\n",
    "                    patterns.append(pattern)\n",
    "            else:\n",
    "                pattern['LOWER'] = pluralize(s.lower())\n",
    "                patterns.append(pattern)\n",
    "        listitem['pattern'] = patterns\n",
    "        termlist.append(listitem)\n",
    "    else:\n",
    "        listitem = None\n",
    "    return termlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create patterns file while also adding ids to tree species objects. If a Wikidata id doesn't exist, add an \n",
    "#auto-incrementing alt_id\n",
    "termlist = []\n",
    "alt_id = 1\n",
    "for t in tree_species:\n",
    "    if 'wikidata' in t:\n",
    "        id = t['wikidata']\n",
    "        t['id'] = t['wikidata']\n",
    "    else:\n",
    "        id = 'x' + str(alt_id)\n",
    "        t['id'] = id\n",
    "    #create the patterns for tree species taxon names\n",
    "    term = constructTerm(t['name'], 'TREE SPECIES', id)\n",
    "    if term is not None:\n",
    "        termlist.extend(term)\n",
    "    #create the patterns for common names\n",
    "    if 'common_names' in t:\n",
    "        for c in t['common_names']:\n",
    "            c_term = constructTerm(str(c), 'TREE COMMON NAME', id)\n",
    "            if c_term is not None:\n",
    "                termlist.extend(c_term)\n",
    "    #create the patterns for alternate species names\n",
    "    if 'alt_species' in t:\n",
    "        for a in t['alt_species']:\n",
    "            a_term = constructTerm(str(a), 'ALT TREE SPECIES', id)\n",
    "            if a_term is not None:\n",
    "                termlist.extend(a_term)\n",
    "    if 'wikidata' not in t:\n",
    "        alt_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_out = open('tree_species.json', 'w')\n",
    "json.dump(tree_species, ts_out)\n",
    "ts_out.close()\n",
    "\n",
    "#save the patterns for SpaCy as a new-line delimited json file (.jsonl)\n",
    "termlistname = 'patterns.jsonl'\n",
    "f = open(termlistname, 'a')\n",
    "writer = jsonlines.Writer(f)\n",
    "writer.write_all(termlist)\n",
    "writer.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Tree Trails book text\n",
    "The full text of Tree Trails is provided by the publisher at https://www.echonyc.com/~parks/books/treetrailsppk.html. We will scrape the html from that page and convert it to plain text with the Python lxml library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = requests.get('https://www.echonyc.com/~parks/books/treetrailsppk.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert html to plain text\n",
    "parser = et.HTMLParser()\n",
    "tree = et.fromstring(query.content, parser)\n",
    "text = et.tostring(tree, method='text', encoding='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few typos in the online version of the text that will affect the text recognition, so let's fix those now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "typos = [{'find': 'Comus florida', 'replace':'Cornus florida'},\n",
    "        {'find': 'anwricana', 'replace':'americana'},\n",
    "        {'find': 'veluntina', 'replace':'velutina'},\n",
    "        {'find': 'Uhnus procera', 'replace':'Ulmus procera'},\n",
    "        {'find': 'Tilia europea', 'replace': 'Tilia europaea'},\n",
    "        {'find': 'P. onentalis', 'replace': 'P. orientalis'},\n",
    "        {'find': 'P. strobits', 'replace': 'P. strobus'}]\n",
    "\n",
    "for t in typos:\n",
    "    text = text.replace(t['find'], t['replace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate species in text\n",
    "This step uses Spacy's [EntityRuler](https://spacy.io/usage/rule-based-matching#entityruler) for rule-based matching on the patterns we created above in the patterns.jsonl file. In this NLP pipeline, we will also identify sentences, so we can group them into paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a blank SpaCy pipeline\n",
    "nlp = spacy.blank('en')\n",
    "#create an instance of the EntityRuler to add to our pipeline below\n",
    "ruler = EntityRuler(nlp)\n",
    "#load the custom vocabs from the appropriate patterns.jsonl\n",
    "patternfile = 'patterns.jsonl'\n",
    "ruler.from_disk(patternfile)\n",
    "#add a pipe in our nlp pipeline to identify the sentences in the text\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "#add a pipe in our nlp pipeline for the EntityRuler to match our patterns\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "#run the text through the nlp pipeline\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entity information from the nlp process. save entities, ids, and character offsets for later use\n",
    "ents = []\n",
    "for e in doc.ents:\n",
    "    ent = {}\n",
    "    ent['text'] = e.text\n",
    "    ent['start_char'] = e.start_char\n",
    "    ent['end_char'] = e.end_char\n",
    "    ent['id'] = e.ent_id_\n",
    "    ent['spacy_id'] = e.ent_id\n",
    "    ent['label'] = e.label_\n",
    "    ents.append(ent)\n",
    "\n",
    "#save sentences and their character offsets in a list\n",
    "sents = []\n",
    "for s in doc.sents:\n",
    "    sent = {}\n",
    "    sent['text'] = s.text\n",
    "    sent['start_char'] = s.start_char\n",
    "    sent['end_char'] = s.end_char\n",
    "    sents.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text and tree data by paragraph\n",
    "With entities and sentences identified, we will now break the text up into its introduction, four tours, and back matter and then group sentences into paragraphs, or \"stops\" on each tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, split text by the word \"TOUR\" and \"FOOTNOTE\". The first segment will be front matter/introduction and the \n",
    "#last segment will be back matter. Everything in between is a tour.\n",
    "sections = []\n",
    "section = []\n",
    "for s in sents:\n",
    "    if ('TOUR' not in s['text']) and ('FOOTNOTE' not in s['text']):\n",
    "        section.append(s)\n",
    "    else:\n",
    "        sections.append(section)\n",
    "        section = []\n",
    "        section.append(s)\n",
    "sections.append(section)\n",
    "\n",
    "#assign tour names to relevant sections\n",
    "tours = {}\n",
    "tours['intro'] = {}\n",
    "tours['intro']['sents'] = sections[0]\n",
    "tours['1'] = {}\n",
    "tours['1']['sents'] = sections[1]\n",
    "tours['2'] = {}\n",
    "tours['2']['sents'] = sections[2]\n",
    "tours['3'] = {}\n",
    "tours['3']['sents'] = sections[3]\n",
    "tours['4'] = {}\n",
    "tours['4']['sents'] = sections[4]\n",
    "tours['back_matter'] = {}\n",
    "tours['back_matter']['sents'] = sections[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tour section, break into paragraphs based on \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split each section into paragraphs based on '\\n\\n' and add to tours dict\n",
    "for k, t in tours.items():\n",
    "    t['paragraphs'] = []\n",
    "    p = {}\n",
    "    p['sents'] = []\n",
    "    for s in t['sents']:\n",
    "        if not(re.match(r'\\n\\n', s['text'])):\n",
    "            p['sents'].append(s)\n",
    "        else:\n",
    "            if len(p['sents']) > 0:\n",
    "                t['paragraphs'].append(p.copy())\n",
    "            p = {}\n",
    "            p['sents'] = []\n",
    "            p['sents'].append(s)\n",
    "    t['paragraphs'].append(p)\n",
    "    #add start and end char offsets for each paragraph\n",
    "    for p in t['paragraphs']:\n",
    "        p['start_char'] = p['sents'][0]['start_char']\n",
    "        p['end_char'] = p['sents'][-1]['end_char']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each paragraph, find the corresponding entities (in tours only) by checking entity character offsets that fall within the paragraph character offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find entities within each paragraph by checking if each entity's starting character offset is within the paragraph offsets\n",
    "for k, t, in tours.items():\n",
    "    #only find entities in the tours, not the introduction or back matter\n",
    "    if k not in ['intro', 'back_matter']:\n",
    "        for p in t['paragraphs']:\n",
    "            p['ents'] = []\n",
    "            for e in ents:\n",
    "                if e['start_char'] in range(p['start_char'], p['end_char']):\n",
    "                    p['ents'].append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be multiple mentions of a species or its common name within a paragraph, so we'll assume they're talking about the same tree and group them. For some common names, if a species isn't reference in a paragraph, we will look it up in the tree_species list and add group it with the common name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommonBySpeciesId(id, tree_species):\n",
    "    common_names = []\n",
    "    for ts in tree_species:\n",
    "        if (ts['id'] == ent['id']):\n",
    "            #check if the matched tree species has common names\n",
    "            if 'common_names' in ts:\n",
    "                for cn in ts['common_names']:\n",
    "                    common_names.append(cn)\n",
    "    return common_names\n",
    "\n",
    "#merge entities with same id within each paragraph, including common names that match tree species in the paragraph and ignoring single common names with no corresponding species\n",
    "for k, t, in tours.items():\n",
    "    for p in t['paragraphs']:\n",
    "        p['merged_ents'] = {}\n",
    "        if 'ents' in p:\n",
    "            for e in p['ents']:\n",
    "              #first add the species\n",
    "              if e['label'] == 'TREE SPECIES':\n",
    "                if e['id'] not in p['merged_ents']:\n",
    "                    p['merged_ents'][e['id']] = [e]\n",
    "                else:\n",
    "                    p['merged_ents'][e['id']].append(e)\n",
    "            for e in p['ents']:\n",
    "              #only include single-token names if they have a corresponding species (single-token name might be too general to be an accurate match)\n",
    "              if e['label'] == 'TREE COMMON NAME':\n",
    "                if ' ' not in e['text']:\n",
    "                    if e['id'] in p['merged_ents']:\n",
    "                        p['merged_ents'][e['id']].append(e)\n",
    "                #if multi-token names have a corresponding species, add to that list\n",
    "                elif e['id'] in p['merged_ents']:\n",
    "                    p['merged_ents'][e['id']].append(e)\n",
    "                else:\n",
    "                    #if not, then check the common name against common names of other species in the paragraph and\n",
    "                    #get the list of entity ids in the paragraph\n",
    "                    common = {}\n",
    "                    #iterate through each id\n",
    "                    for ent in p['ents']:\n",
    "                        if ent['label'] == 'TREE SPECIES':\n",
    "                            if len(common) == 0:\n",
    "                                cn = getCommonBySpeciesId(ent['id'], tree_species)\n",
    "                                for c in cn:\n",
    "                                    #title case the name and check if it matches the singular or plural form of the common name\n",
    "                                    if e['text'].title().replace(\"'S\", \"'s\") in [pluralize(c.title().replace(\"'S\", \"'s\")), c.title().replace(\"'S\", \"'s\")]:\n",
    "                                        #if so, add it and its tree species to the list\n",
    "                                        common = [{'text':e['text'], 'label':'TREE COMMON NAME', 'id':ent['id'], 'start_char':e['start_char']},\n",
    "                                                  {'text':ent['label'], 'label':'TREE SPECIES', 'id':ent['id']}]\n",
    "                                        if ent['id'] in p['merged_ents']:\n",
    "                                            p['merged_ents'][ent['id']].extend(common)\n",
    "                                        else:\n",
    "                                            p['merged_ents'][ent['id']] = common\n",
    "                    #otherwise, check the common names of species in the paragraph against tree_species\n",
    "                    if len(common) == 0:\n",
    "                        species = {}\n",
    "                        for ts in tree_species:\n",
    "                            if e['id'] == ts['id']:\n",
    "                                species = [{'text':ts['name'], 'label':'TREE SPECIES', 'id':e['id']},\n",
    "                                           {'text':e['text'], 'label':'TREE COMMON NAME', 'id':e['id'], 'start_char':e['start_char']}]\n",
    "                        p['merged_ents'][e['id']] = species\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tour \"stops\" with title, lead-in, rich text book excerpt, tour number, and id to use for linking to tree species list\n",
    "For each tour stop entry in the database, we want a title (the taxon name of the tree), a lead-in to display below the title (the first 35 characters of the paragraph), the book excerpt paragraph with tree species bold and italic, common names italic and both displayed in a difference color, tour number for filtering, and the species name to link to the tree species dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of the `formatting` value below to 'airtable' if you want to output CSV files for import into Airtable, which doesn't allow for rich text, instead of Memento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting = 'memento'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(text, formatting='memento'):\n",
    "    if formatting == 'memento':\n",
    "        text = '<b>' + text + '</b>'\n",
    "    return text\n",
    "\n",
    "def italic(text, rgb='156, 39, 176', formatting='memento'):\n",
    "    if formatting == 'memento':   \n",
    "        if rgb is not None:\n",
    "            color = 'style=\"color: rgb({});\"'.format(rgb)\n",
    "            text = '<i {}>'.format(color) + text + '</i>'\n",
    "        else:\n",
    "            text = '<i>' + text + '</i>'\n",
    "    return text\n",
    "\n",
    "def lineBreaks(text, formatting='memento'):\n",
    "    \"\"\"Format line breaks consistently\"\"\"\n",
    "    #strip line breaks at the start of a stop\n",
    "    lbs = re.compile('\\xa0')\n",
    "    text = re.sub(lbs, '', text)\n",
    "    stopstart = re.compile('^\\n+')\n",
    "    text = re.sub(stopstart, '', text)\n",
    "    #replace excessive linebreaks with double line break\n",
    "    lb = re.compile('\\n\\n+')\n",
    "    text = re.sub(lb, '\\n\\n', text)\n",
    "    return text\n",
    "\n",
    "def joinSents(sents):\n",
    "    \"\"\"Join sentences with consistent spacing\"\"\"\n",
    "    text = ' '.join([s['text'] for s in sents])\n",
    "    spacing = re.compile(' +')\n",
    "    text = re.sub(spacing, ' ', text)\n",
    "    return text\n",
    "\n",
    "def createTitle(merged_ent):\n",
    "    \"\"\"Make the title the common name followed by the species in parentheses, or just the species, if the common name \n",
    "    doesn't appear in the paragraph\"\"\"\n",
    "    species = None\n",
    "    commons = []\n",
    "    for m in merged_ent:\n",
    "        #add any common names to a list\n",
    "        if m['label'] == 'TREE COMMON NAME':\n",
    "            commons.append(m['text'])\n",
    "        #get full tree species name\n",
    "        for ts in tree_species:\n",
    "            if m['id'] == ts['id']:\n",
    "                species = ts['name']\n",
    "    commons = list(set(commons))  \n",
    "    if len(commons) > 0:\n",
    "        #use the first common name in the common name list as the title\n",
    "        title = commons[0].capitalize()\n",
    "        #add the species in parentheses after the common name\n",
    "        if species is not None:\n",
    "            title = title + ' (' + species.capitalize() + ')'\n",
    "    elif species is not None:\n",
    "        title = species.capitalize()\n",
    "    return title\n",
    "\n",
    "def createExcerpt(paragraph, merged_ent):\n",
    "    \"\"\"Convert paragraphs into rich text, bolding and/or italicizing tree names\"\"\"\n",
    "    #get unique entities and labels\n",
    "    u_ents = {}\n",
    "    for m in merged_ent:\n",
    "        if m['label'] in u_ents:\n",
    "            if m['text'] not in u_ents[m['label']]:\n",
    "                u_ents[m['label']].append(m['text'])\n",
    "        else:\n",
    "            u_ents[m['label']] = [m['text']]\n",
    "    p_text = joinSents(paragraph['sents'])\n",
    "    #join sentences\n",
    "    excerpt = lineBreaks(p_text, formatting=formatting)\n",
    "    for k, u in u_ents.items():\n",
    "        if k in ['TREE SPECIES', 'ALT TREE SPECIES']:\n",
    "            for text in u:\n",
    "                excerpt = excerpt.replace(text, bold(italic(text, formatting=formatting), formatting=formatting))\n",
    "        if k == 'TREE COMMON NAME':\n",
    "            for text in u:\n",
    "                excerpt = excerpt.replace(text, italic(text, formatting=formatting))\n",
    "    return excerpt\n",
    "\n",
    "def getSpecies(merged_ent, tree_species):\n",
    "    \"\"\"Get the tree species name for an entity\"\"\"\n",
    "    species = None\n",
    "    for m in merged_ent:\n",
    "        if m['label'] == 'TREE SPECIES':\n",
    "            for ts in tree_species:\n",
    "                if m['id'] == ts['id']:\n",
    "                    species = ts['name']\n",
    "    return species\n",
    "\n",
    "def createLeadIn(p):\n",
    "    \"\"\"Use the first 35 characters as a lead-in field to use in the card description\"\"\"\n",
    "    leadin = p['sents'][0]['text'].replace('\\n', ' ').strip()[0:35] + '...'\n",
    "    return leadin\n",
    "\n",
    "def createStop(paragraph, merged_ent, id, tree_species):\n",
    "    \"\"\"Create all the database fields for a tour stop for each merged entity in a paragraph\"\"\"\n",
    "    stop = {}\n",
    "    stop['title'] = createTitle(merged_ent)\n",
    "    stop['lead-in'] = createLeadIn(paragraph)\n",
    "    stop['excerpt'] = createExcerpt(paragraph, merged_ent)\n",
    "    stop['species'] = getSpecies(merged_ent, tree_species)\n",
    "    return stop\n",
    "\n",
    "def appendNoEntPara(stops, p):\n",
    "    \"\"\"If there are no entities in a paragraph, append it to the previous stop (or stops if the last para was\n",
    "    broken up into multiple stops)\"\"\"\n",
    "    prev_stop = -2\n",
    "    p_text = '\\n\\n' + joinSents(p['sents'])\n",
    "    stops[-1]['excerpt'] = lineBreaks(stops[-1]['excerpt'] + p_text, formatting=formatting)\n",
    "    #append it to all previous stops with the same lead-in, for previous paragraphs repeated for multiple entities\n",
    "    if len(stops) > 1:\n",
    "        while stops[prev_stop]['lead-in'] ==  stops[-1]['lead-in']:\n",
    "            #re-run it through lineBreaks() after adding to remove any errant formatting\n",
    "            stops[prev_stop]['excerpt'] = lineBreaks(stops[prev_stop]['excerpt'] + p_text, formatting=formatting)  \n",
    "            prev_stop = prev_stop - 1\n",
    "    return stops\n",
    "\n",
    "def writeStopsToCsv(stops, out):\n",
    "    \"\"\"Write the data out to a csv file suitable for import into the database\"\"\"\n",
    "    fieldnames = ['Name', 'Description', 'Excerpt', 'Tree species', 'Tour', 'Sequence']\n",
    "    writer = csv.DictWriter(out, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for s in stops:\n",
    "        row = {}\n",
    "        row['Name'] = s['title']\n",
    "        row['Excerpt'] = s['excerpt']\n",
    "        row['Description'] = s['lead-in']\n",
    "        if 'species' in s:\n",
    "            row['Tree species'] = s['species']\n",
    "        row['Tour'] = s['tour']\n",
    "        row['Sequence'] = s['sequence']\n",
    "        writer.writerow(row)\n",
    "    out.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tour \"stops\" for each merged entity in each paragraph\n",
    "Iterate through each paragraph in each tour. For the Introduction, all paragraphs will go into one stop, after a little extra data clean-up. For Tours, if there are multiple different tree species in a paragraph, the paragraph will get repeated as a stop for each tree species, so that you can add separate geocoordinates and images for each tree. If a paragraph has no tree species, then it will be appended to the previous stop (or stops, if the previous paragraph contained multiple tree species). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process front matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, t in tours.items():\n",
    "    tour = k\n",
    "    #for title page/colophon \n",
    "    if k == 'intro':\n",
    "        fm_p = None\n",
    "        #find all front matter before the table of contents and the Marianne Moore poem, which is under a different copyright\n",
    "        for i, p in enumerate(t['paragraphs']):\n",
    "            fmp_text = joinSents(p['sents'])\n",
    "            if 'TABLE' in fmp_text:\n",
    "                fm_p = i\n",
    "        front_matter_p = t['paragraphs'][0:fm_p]\n",
    "        #only use text after the web page header\n",
    "        for i, fs in enumerate(front_matter_p):\n",
    "            for j, fss in enumerate(fs['sents']):\n",
    "                if 'Tree Trails in Prospect Park' in fss['text']:\n",
    "                    front_matter_p[i]['sents'][i]['text'] = 'Tree Trails in Prospect Park' + front_matter_p[i]['sents'][i]['text'].split('Tree Trails in Prospect Park')[1]\n",
    "        fm_texts = []\n",
    "        #join all sentences in the front matter, omitting any text up to and including 'TABLE'\n",
    "        for fmp in front_matter_p:\n",
    "            fm_text = [s['text'] for s in fmp['sents']]\n",
    "            for f in fm_text:\n",
    "                if 'TABLE' in f:\n",
    "                    f = f.split('TABLE')[0]\n",
    "                fm_texts.append(f) \n",
    "        fm = lineBreaks(''.join(fm_texts), formatting=formatting)\n",
    "        #create the stop for the front_matter\n",
    "        stop = {}\n",
    "        stop['title'] = 'Front Matter'\n",
    "        stop['lead-in'] = fm[0:30] + '...'\n",
    "        stop['excerpt'] = fm\n",
    "        stop['tour'] = 'Introduction'\n",
    "        stops.append(stop.copy())\n",
    "        #get the paragraphs in the introduction, after the poem\n",
    "        intro_p = None\n",
    "        #find the paragraph with \"INTRODUCTION\", so we can use all text after that for the intro\n",
    "        for i, p in enumerate(t['paragraphs']):\n",
    "            p_text = joinSents(p['sents'])\n",
    "            if 'INTRODUCTION' in p_text:\n",
    "                intro_p = i\n",
    "        intro_paragraphs = t['paragraphs'][intro_p:]\n",
    "        texts = []\n",
    "        #join all sentences in the intro, omitting any text up to and including 'INTRODUCTION'\n",
    "        for p in intro_paragraphs:\n",
    "            text = [s['text'] for s in p['sents']]\n",
    "            for t in text:\n",
    "                if 'INTRODUCTION' in t:\n",
    "                    t = t.split('INTRODUCTION')[1]\n",
    "                texts.append(t)   \n",
    "        #join while cleaning up line breaks and whitespace\n",
    "        p_text = lineBreaks(''.join(texts))\n",
    "        #create the stops for the intro\n",
    "        stop = {}\n",
    "        stop['title'] = 'INTRODUCTION'\n",
    "        stop['lead-in'] = p_text[0:35] + '...'\n",
    "        stop['excerpt'] = p_text\n",
    "        stop['tour'] = 'Introduction'\n",
    "        stops.append(stop.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, t in tours.items():\n",
    "    tour = k\n",
    "    #for intro \n",
    "    if k not in ['intro', 'back_matter']:\n",
    "        for p in t['paragraphs']:\n",
    "            if 'merged_ents' not in p:\n",
    "                p['merged_ents'] = {}\n",
    "            #if there are no entities in the paragraph, then append the paragraph text to the previous stop excerpt, unless it contains \"TOUR\"\n",
    "            if len(p['merged_ents']) == 0:\n",
    "                if 'TOUR' not in joinSents(p['sents']):\n",
    "                    if len(stops) > 0:\n",
    "                        stops = appendNoEntPara(stops, p)\n",
    "                else:\n",
    "                    stop = {}\n",
    "                    stop['title'] = 'TOUR ' + k            \n",
    "                    p_text = joinSents(p['sents'])\n",
    "                    #if there is other text in this paragraph before the tour name, split it out and append it to the previous stop(s)\n",
    "                    if 'TOUR' in p_text:\n",
    "                        p_text = 'TOUR' + p_text.split('TOUR')[1]\n",
    "                        to_prev = {'sents':[{'text':p_text.split('TOUR')[0]}]}\n",
    "                        appendNoEntPara(stops, to_prev)\n",
    "                    stop['lead-in'] = lineBreaks(p_text[0:35] + '...', formatting=formatting)\n",
    "                    stop['excerpt'] = lineBreaks(p_text, formatting=formatting)\n",
    "                    stop['tour'] = 'TOUR ' + tour\n",
    "                    stops.append(stop.copy())\n",
    "            #create stop for each merged ent in a paragraph\n",
    "            else:\n",
    "                if 'merged_ents' in p:\n",
    "                    #order merged_ents by earliest offsets\n",
    "                    merged_ents = []\n",
    "                    ordered_ents = []\n",
    "                    for k, v in p['merged_ents'].items():\n",
    "                        if len(v) > 0:\n",
    "                            m_ent = {}\n",
    "                            m_ent['id'] = k\n",
    "                            m_ent['earliest_start_char'] = min([d['start_char'] for d in v if 'start_char' in d])\n",
    "                            m_ent['ents'] = v\n",
    "                            merged_ents.append(m_ent)\n",
    "                        ordered_ents = sorted(merged_ents, key=lambda k: k['earliest_start_char']) \n",
    "                    for o in ordered_ents:\n",
    "                        stop = createStop(p, o['ents'], o['id'], tree_species)\n",
    "                        stop['tour'] = 'TOUR ' + tour\n",
    "                        stops.append(stop.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process back matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, t in tours.items():\n",
    "    tour = k\n",
    "    #for back matter\n",
    "    if k == 'back_matter':\n",
    "        bm_p = None\n",
    "        #split out the FOOTNOTE... and WORD ABOUT THE AUTHOR into two sections\n",
    "        footnote = []\n",
    "        wordabout = []\n",
    "        #find the paragraph indexes for each\n",
    "        for i, p in enumerate(t['paragraphs']):\n",
    "            bmp_text = joinSents(p['sents'])\n",
    "            if 'FOOTNOTE' in bmp_text:\n",
    "                fn_p = i\n",
    "            if 'WORD ABOUT' in bmp_text:\n",
    "                wa_p = i\n",
    "        #split by index and add to separate lists\n",
    "        for i, p in enumerate(t['paragraphs'][0:wa_p]):\n",
    "            fnp_text = ''.join([s['text'] for s in p['sents']])\n",
    "            if 'FOOTNOTE' in fnp_text:\n",
    "                #add the text before the 'FOOTNOTE' to the previous stop's excerpt\n",
    "                to_prev = {'sents':[{'text':fnp_text.split('FOOTNOTE')[0]}]}\n",
    "                appendNoEntPara(stops, to_prev)\n",
    "                #add the rest to the footnotes list of paragraphs\n",
    "                fnp_text_rest = {'sents':[{'text':'FOOTNOTE' + fnp_text.split('FOOTNOTE')[1]}]}\n",
    "                footnote.append(joinSents(fnp_text_rest['sents']))\n",
    "            else:\n",
    "                footnote.append(joinSents(p['sents']))\n",
    "        #join all of the sents for the footnote\n",
    "        fn = lineBreaks(''.join(footnote), formatting=formatting)\n",
    "        #create the stop for the footnote\n",
    "        stop = {}\n",
    "        stop['title'] = 'FOOTNOTE TO TREE TRAILS'\n",
    "        stop['lead-in'] = fn[0:35] + '...'\n",
    "        stop['excerpt'] = fn\n",
    "        stop['tour'] = 'Back matter'\n",
    "        stops.append(stop.copy())\n",
    "        #process the WORD ABOUT\n",
    "        for i, p in enumerate(t['paragraphs'][wa_p:]):\n",
    "            wap_text = joinSents(p['sents'])\n",
    "            if 'A WORD ABOUT' in wap_text:\n",
    "                #add the text before 'A WORD ABOUT' to the previous stop's excerpt\n",
    "                to_prev = {'sents':[{'text':wap_text.split('A WORD ABOUT')[0]}]}\n",
    "                appendNoEntPara(stops, to_prev)\n",
    "                #add the rest to the footnotes list of paragraphs\n",
    "                wap_text_rest = {'sents':[{'text':'A WORD ABOUT' + wap_text.split('A WORD ABOUT')[1]}]}\n",
    "                wordabout.append(joinSents(wap_text_rest['sents']))\n",
    "            else:\n",
    "                wordabout.append(joinSents(p['sents']))\n",
    "        #join all of the sents\n",
    "        wa = lineBreaks(' '.join(wordabout), formatting=formatting)\n",
    "        #create the stop for the word about the author\n",
    "        stop = {}\n",
    "        stop['title'] = 'A WORD ABOUT THE AUTHOR'\n",
    "        stop['lead-in'] = wa[0:35] + '...'\n",
    "        stop['excerpt'] = lineBreaks(wa.replace('Top of page', ''), formatting=formatting)\n",
    "        stop['tour'] = 'Back matter'\n",
    "        stops.append(stop.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make final edits to stops\n",
    "Some stops are just references to other trees and not about the actual trees on the tour, so we should delete these. I compiled a list manually in the 'pp-tree-trails_deletes.json' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripMarkup(text):\n",
    "    \"\"\"Remove HTML and markdown markup from text\"\"\"\n",
    "    text = text.replace('**', '').replace('_', '')\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the manually created list of entries to delete\n",
    "d = open('pp-tree-trails_deletes.json', 'r')\n",
    "deletes = json.load(d)\n",
    "\n",
    "#check the lead-in and species of each stop against the list of deletes and add to a new final_stops list if not in the deletes list\n",
    "final_stops = []\n",
    "for i, s in enumerate(stops):\n",
    "    if not any((s['lead-in'] == d['lead-in']) and (s['species'] == d['species']) for d in deletes):\n",
    "        final_stops.append(s)\n",
    "    #if deleting the stop means deleting the only instance of the paragraph(s), then remove the formatting and \n",
    "    #add it to the previous stop\n",
    "    else:\n",
    "        if final_stops[-1]['lead-in'] != s['lead-in'] and stops[i+1]['lead-in'] != s['lead-in']:\n",
    "            final_stops[-1]['excerpt'] = final_stops[-1]['excerpt'] + '\\n' + stripMarkup(s['excerpt'])\n",
    "            #if there is more than one stop with that lead-in, add it to all of them\n",
    "            if len(final_stops) > 1:\n",
    "                prev_stop = -2\n",
    "                while final_stops[prev_stop]['lead-in'] ==  final_stops[-1]['lead-in']:\n",
    "                    final_stops[prev_stop]['excerpt'] = final_stops[prev_stop]['excerpt'] + '\\n' + stripMarkup(s['excerpt'])  \n",
    "                    prev_stop = prev_stop - 1\n",
    "        \n",
    "#add sequence numbers in case the list needs to get resorted\n",
    "seq = 1\n",
    "for f in final_stops:\n",
    "    f['sequence'] = seq\n",
    "    seq += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the final tour stop list out ot CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to CSV for import into app\n",
    "stop_out = open('tree_trails.csv', 'w')\n",
    "writeStopsToCsv(final_stops, stop_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the tree species list to only those that appear in the guide and write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree_species = []\n",
    "sections = [t['paragraphs'] for k, t, in tours.items()]\n",
    "for ts in tree_species:\n",
    "    in_final = False\n",
    "    for s in sections:\n",
    "        for p in s:\n",
    "            for k, m in p['merged_ents'].items():\n",
    "                if ts['id'] == k:\n",
    "                    if in_final == False:\n",
    "                        final_tree_species.append(ts)\n",
    "                        in_final = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeSpeciesToCsv(final_tree_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data into Memento or Airtable\n",
    "Follow the steps in [importing-database-data.md](importing-database-data.md) to create the Memento or Airtable databases on your Android phone and import the data from 'tree_species.csv' and 'tree_trails.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
